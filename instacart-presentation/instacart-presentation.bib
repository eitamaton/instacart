
@article{quadranaSequenceAwareRecommenderSystems2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.08452},
  primaryClass = {cs},
  title = {Sequence-{{Aware Recommender Systems}}},
  abstract = {Recommender systems are one of the most successful applications of data mining and machine learning technology in practice. Academic research in the field is historically often based on the matrix completion problem formulation, where for each user-item-pair only one interaction (e.g., a rating) is considered. In many application domains, however, multiple user-item interactions of different types can be recorded over time. And, a number of recent works have shown that this information can be used to build richer individual user models and to discover additional behavioral patterns that can be leveraged in the recommendation process. In this work we review existing works that consider information from such sequentially-ordered user- item interaction logs in the recommendation process. Based on this review, we propose a categorization of the corresponding recommendation tasks and goals, summarize existing algorithmic solutions, discuss methodological approaches when benchmarking what we call sequence-aware recommender systems, and outline open challenges in the area.},
  urldate = {2019-04-03},
  journal = {arXiv:1802.08452 [cs]},
  url = {http://arxiv.org/abs/1802.08452},
  author = {Quadrana, Massimo and Cremonesi, Paolo and Jannach, Dietmar},
  month = feb,
  year = {2018},
  keywords = {recommender-systems,Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval,instacart},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/SD55GRE8/Quadrana et al. - 2018 - Sequence-Aware Recommender Systems.pdf;/Users/i/Documents/Application Support/Zotero/Work/storage/DPN25RRC/1802.html}
}

@inproceedings{rendleFactorizingPersonalizedMarkov2010,
  address = {{Raleigh, North Carolina, USA}},
  title = {Factorizing Personalized {{Markov}} Chains for Next-Basket Recommendation},
  isbn = {978-1-60558-799-8},
  abstract = {Recommender systems are an important component of many websites. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. On the other hand, MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized transition graphs over underlying Markov chains. That means for each user an own transition matrix is learned \textendash{} thus in total the method uses a transition cube. As the observations for estimating the transitions are usually very limited, our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition. We show that our factorized personalized MC (FPMC) model subsumes both a common Markov chain and the normal matrix factorization model. For learning the model parameters, we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data. Empirically, we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization.},
  language = {en},
  urldate = {2019-04-03},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web - {{WWW}} '10},
  publisher = {{ACM Press}},
  doi = {10.1145/1772690.1772773},
  url = {http://portal.acm.org/citation.cfm?doid=1772690.1772773},
  author = {Rendle, Steffen and Freudenthaler, Christoph and {Schmidt-Thieme}, Lars},
  year = {2010},
  keywords = {instacart},
  pages = {811},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/Z8LAWDWM/Rendle et al. - 2010 - Factorizing personalized Markov chains for next-ba.pdf}
}

@article{guerraouiSequencesItemsLatent2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.06100},
  primaryClass = {cs, stat},
  title = {Sequences, {{Items And Latent Links}}: {{Recommendation With Consumed Item Packs}}},
  shorttitle = {Sequences, {{Items And Latent Links}}},
  abstract = {Recommenders personalize the web content by typically using collaborative filtering to relate users (or items) based on explicit feedback, e.g., ratings. The difficulty of collecting this feedback has recently motivated to consider implicit feedback (e.g., item consumption along with the corresponding time).},
  language = {en},
  urldate = {2019-04-03},
  journal = {arXiv:1711.06100 [cs, stat]},
  url = {http://arxiv.org/abs/1711.06100},
  author = {Guerraoui, Rachid and Merrer, Erwan Le and Patra, Rhicheek and Vigouroux, Jean-Ronan},
  month = nov,
  year = {2017},
  keywords = {Statistics - Machine Learning,Computer Science - Information Retrieval,Computer Science - Social and Information Networks,instacart},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/BPIGHHHV/Guerraoui et al. - 2017 - Sequences, Items And Latent Links Recommendation .pdf}
}

@inproceedings{liuRepeatBuyerPrediction2016,
  address = {{San Francisco, California, USA}},
  title = {Repeat {{Buyer Prediction}} for {{E}}-{{Commerce}}},
  isbn = {978-1-4503-4232-2},
  abstract = {A large number of new buyers are often acquired by merchants during promotions. However, many of the attracted buyers are one-time deal hunters, and the promotions may have little long-lasting impact on sales. It is important for merchants to identify who can be converted to regular loyal buyers and then target them to reduce promotion cost and increase the return on investment (ROI). At International Joint Conferences on Artificial Intelligence (IJCAI) 2015, Alibaba hosted an international competition for repeat buyer prediction based on the sales data of the ``Double 11'' shopping event in 2014 at Tmall.com. We won the first place at stage 1 of the competition out of 753 teams. In this paper, we present our winning solution, which consists of comprehensive feature engineering and model training. We created profiles for users, merchants, brands, categories, items and their interactions via extensive feature engineering. These profiles are not only useful for this particular prediction task, but can also be used for other important tasks in e-commerce, such as customer segmentation, product recommendation, and customer base augmentation for brands. Feature engineering is often the most important factor for the success of a prediction task, but not much work can be found in the literature on feature engineering for prediction tasks in e-commerce. Our work provides some useful hints and insights for data science practitioners in e-commerce.},
  language = {en},
  urldate = {2019-04-07},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  publisher = {{ACM Press}},
  doi = {10.1145/2939672.2939674},
  url = {http://dl.acm.org/citation.cfm?doid=2939672.2939674},
  author = {Liu, Guimei and Nguyen, Tam T. and Zhao, Gang and Zha, Wei and Yang, Jianbo and Cao, Jianneng and Wu, Min and Zhao, Peilin and Chen, Wei},
  year = {2016},
  keywords = {instacart},
  pages = {155-164},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/DZ8N53U8/Liu et al. - 2016 - Repeat Buyer Prediction for E-Commerce.pdf}
}

@article{bleiLatentDirichletAllocation2003,
  title = {Latent {{Dirichlet Allocation}}},
  volume = {3},
  issn = {ISSN 1533-7928},
  number = {Jan},
  urldate = {2019-06-16},
  journal = {Journal of Machine Learning Research},
  url = {http://jmlr.csail.mit.edu/papers/v3/blei03a.html},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  keywords = {instacart,lda},
  pages = {993-1022},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/PU6RN5MK/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf;/Users/i/Documents/Application Support/Zotero/Work/storage/CFELUP8D/blei03a.html}
}

@article{Hoffman:2013:SVI:2502581.2502622,
  title = {Stochastic {{Variational Inference}}},
  volume = {14},
  issn = {1532-4435},
  number = {1},
  journal = {J. Mach. Learn. Res.},
  url = {http://dl.acm.org/citation.cfm?id=2502581.2502622},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  month = may,
  year = {2013},
  keywords = {instacart,Bayesian inference,Bayesian nonparametrics,stochastic optimization,topic models,variational inference,lda},
  pages = {1303-1347},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/HHRY4AUP/Hoffman - Stochastic Variational Inference.pdf},
  publisher = {{JMLR.org}},
  issue_date = {January 2013},
  numpages = {45},
  acmid = {2502622}
}

@inproceedings{jeniFacingImbalancedData2013,
  address = {{Geneva, Switzerland}},
  title = {Facing {{Imbalanced Data}}--{{Recommendations}} for the {{Use}} of {{Performance Metrics}}},
  isbn = {978-0-7695-5048-0},
  abstract = {Recognizing facial action units (AUs) is important for situation analysis and automated video annotation. Previous work has emphasized face tracking and registration and the choice of features classifiers. Relatively neglected is the effect of imbalanced data for action unit detection. While the machine learning community has become aware of the problem of skewed data for training classifiers, little attention has been paid to how skew may bias performance metrics. To address this question, we conducted experiments using both simulated classifiers and three major databases that differ in size, type of FACS coding, and degree of skew. We evaluated influence of skew on both threshold metrics (Accuracy, F-score, Cohen's kappa, and Krippendorf's alpha) and rank metrics (area under the receiver operating characteristic (ROC) curve and precision-recall curve). With exception of area under the ROC curve, all were attenuated by skewed distributions, in many cases, dramatically so. While ROC was unaffected by skew, precision-recall curves suggest that ROC may mask poor performance. Our findings suggest that skew is a critical factor in evaluating performance metrics. To avoid or minimize skewbiased estimates of performance, we recommend reporting skew-normalized scores along with the obtained ones.},
  language = {en},
  urldate = {2019-06-16},
  booktitle = {2013 {{Humaine Association Conference}} on {{Affective Computing}} and {{Intelligent Interaction}}},
  publisher = {{IEEE}},
  doi = {10.1109/ACII.2013.47},
  url = {http://ieeexplore.ieee.org/document/6681438/},
  author = {Jeni, Laszlo A. and Cohn, Jeffrey F. and De La Torre, Fernando},
  month = sep,
  year = {2013},
  keywords = {instacart},
  pages = {245-251},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/JSHC9Y76/Jeni et al. - 2013 - Facing Imbalanced Data--Recommendations for the Us.pdf}
}

@techreport{chenUsingRandomForest2004,
  address = {{Department of Statistics}},
  title = {Using {{Random Forest}} to {{Learn Imbalanced Data}}},
  abstract = {In this paper we propose two ways to deal with the imbalanced data classification problem using random forest. One is based on cost sensitive learning, and the other is based on a sampling technique. Performance metrics such as precision and recall, false positive rate and false negative rate, F-measure and weighted accuracy are computed. Both methods are shown to improve the prediction accuracy of the minority class, and have favorable performance compared to the existing algorithms.},
  language = {en},
  number = {666},
  institution = {{University of California, Berkeley}},
  url = {https://statistics.berkeley.edu/tech-reports/666},
  author = {Chen, Chao and {Andy Liaw} and {Leo Breiman}},
  month = jul,
  year = {2004},
  keywords = {instacart},
  pages = {12},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/XEZP7AKQ/Chen - Using Random Forest to Learn Imbalanced Data.pdf}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  volume = {45},
  issn = {1573-0565},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash{}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  language = {en},
  number = {1},
  urldate = {2019-06-19},
  journal = {Machine Learning},
  doi = {10.1023/A:1010933404324},
  url = {https://doi.org/10.1023/A:1010933404324},
  author = {Breiman, Leo},
  month = oct,
  year = {2001},
  keywords = {machine-learning,instacart,classification,ensemble,regression},
  pages = {5-32},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/IBPV7M3H/Breiman - 2001 - Random Forests.pdf}
}

@book{hastieElementsStatisticalLearning2009,
  address = {{New York, NY}},
  series = {Springer {{Series}} in {{Statistics}}},
  title = {The {{Elements}} of {{Statistical Learning}}},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  urldate = {2019-06-19},
  publisher = {{Springer New York}},
  url = {http://link.springer.com/10.1007/978-0-387-84858-7},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  keywords = {machine-learning,instacart},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/YKVR2JUF/Elements of Statistical Learning data mining, inf.pdf},
  doi = {10.1007/978-0-387-84858-7}
}

@misc{brownleeHowWhenUse2018,
  title = {How and {{When}} to {{Use ROC Curves}} and {{Precision}}-{{Recall Curves}} for {{Classification}} in {{Python}}},
  abstract = {It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly. This flexibility comes from the way that probabilities may be interpreted using different thresholds that allow the operator of the model to trade-off concerns in the errors made by the model, \ldots{}},
  language = {en-US},
  urldate = {2019-06-25},
  journal = {Machine Learning Mastery},
  url = {https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/},
  author = {Brownlee, Jason},
  month = aug,
  year = {2018},
  keywords = {instacart,metrics},
  file = {/Users/i/Documents/Application Support/Zotero/Work/storage/2L84PQNT/roc-curves-and-precision-recall-curves-for-classification-in-python.html}
}


